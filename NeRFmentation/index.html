<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="SyntheticData4CV 2024 Paper">
  <meta property="og:title" content="NeRFmentation" />
  <meta property="og:description" content="Improving Monocular Depth Estimation with NeRF-based Data Augmentation" />
  <meta property="og:url" content="https://casimirfeldmann.github.io/NeRFmentation" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="SyntheticData4CV 2024 Paper">
  <meta name="twitter:description" content="NeRFmentation">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>NeRFmentation: Improving Monocular Depth Estimation with NeRF-based Data Augmentation</title>
  <link rel="icon" type="image/x-icon" href="static/images/nerfmentation_logo.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/styles.css" />
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="static/css/image_comparison_grid.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/js/all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/index.js"></script>
  <script src="static/js/index.js"></script>

</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">NeRFmentation: Improving Monocular Depth Estimation with NeRF-based
              Data Augmentation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://casimirfeldmann.github.io" target="_blank">Casimir Feldmann</a><sup>*1</sup>,</span>
              <span class="author-block">
                <a href="https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://ch.linkedin.com/in/niall-siegenheim"
                  target="_blank">Niall Siegenheim</a><sup>*1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=r4vUa20AAAAJ&hl=en" target="_blank">Nikolas
                  Hars</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=xJeEb2AAAAAJ&hl=en" target="_blank">Lovro
                  Rabuzin</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://edelmanlab.mit.edu/current-members/mert-ertugrul/"
                  target="_blank">Mert Ertugrul</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://ch.linkedin.com/in/luca-wolfart-3b9a9b1bb"
                  target="_blank">Luca Wolfart</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://people.inf.ethz.ch/marc.pollefeys/" target="_blank">Marc
                  Pollefeys</a><sup>1,2</sup>,</span>
              <span class="author-block">
                <a href="https://zuriabauer.com/" target="_blank">Zuria Bauer</a><sup>1,3</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.de/citations?user=biytQP8AAAAJ&hl=en" target="_blank">Martin R.
                  Oswald</a><sup>1,4</sup>,</span>
              </span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">ETH Zürich</a><sup>1</sup>, Microsoft</a><sup>2</sup>, University of
                Alicante</a><sup>3</sup>, University of Amsterdam</a><sup>4</sup> <br>SyntheticData4CV 2024 (ECCV
                Workshop) </span>
            </div>
            <div class="is-size-5 publication-authors">
              <sup>1</sup> <a href="https://ethz.ch"><img style="width:20%; padding-right: 15px;"
                  src="static/images/eth_logo.png"> </a>
              <sup>2</sup> <a href="https://www.microsoft.com"><img style="width:20%; padding-right: 15px;"
                  src="static/images/microsoft_logo.jpg"> </a>
              <sup>3</sup> <a href="https://www.ua.es"><img style="width:12%; padding-right: 15px;"
                  src="static/images/UniversityAlicanteLogo.png"> </a>
              <sup>4</sup> <a href="https://www.uva.nl"><img style="width:20%; padding-right: 15px;"
                  src="static/images/university-of-amsterdam.png"> </a>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2401.03771v2" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                <!-- Github link -->
                <!-- <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming Soon)</span>
                  </a>
                </span> -->

                <!-- Github link (disabled and greyed out) -->
                <span class="link-block">
                  <a class="external-link button is-normal is-rounded is-light"
                    style="cursor: default; pointer-events: none; opacity: 0.6;">
                    <span class="icon" style="color: #4a4a4a;">
                      <i class="fab fa-github"></i>
                    </span>
                    <span style="color: #4a4a4a;">Code (Coming Soon)</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span> -->

                <!-- Video Link. -->
                <span class="link-block">
                  <a href="https://www.youtube.com/watch?v=ICOT_zkTqmI"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>


                <!-- Dataset Link. -->
                <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span>  -->

                <!-- Dataset Link. (disabled and greyed out) -->
                <span class="link-block">
                  <a class="external-link button is-normal is-rounded is-light"
                    style="cursor: default; pointer-events: none; opacity: 0.6;">
                    <span class="icon" style="color: #4a4a4a;">
                      <i class="far fa-images"></i>
                    </span>
                    <span style="color: #4a4a4a;">Data (Coming Soon)</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <!-- Your video here -->
          <source src="static/videos/reconstructed_kitti_teaser.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          We propose a novel dataset augmentation pipeline that utilizes NeRF-generated data rendered from poses unseen
          in the original dataset to improve the robustness of Monocular Depth Estimation networks. Here are novel
          interpolated NeRF
          RGB and depth renders of the KITTI dataset.
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              The capabilities of monocular depth estimation (MDE) models are limited by the availability of sufficient
              and diverse datasets. In the case of MDE models for autonomous driving, this issue is exacerbated by the
              linearity of the captured data trajectories. We propose a NeRF-based data augmentation pipeline to
              introduce synthetic data with more diverse viewing directions into training datasets and demonstrate the
              benefits of our approach to model performance and robustness. Our data augmentation pipeline, which we
              call <strong>NeRFmentation</strong>, trains NeRFs on each scene in a dataset, filters out subpar NeRFs
              based on
              relevant metrics, and uses them to generate synthetic RGB-D images captured from new viewing directions.
              In this work, we apply our technique in conjunction with three state-of-the-art MDE architectures on the
              popular autonomous driving dataset, KITTI, augmenting its training set of the Eigen split. We evaluate the
              resulting performance gain on the original test set, a separate popular driving dataset, and our own
              synthetic test set.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Image Comparison -->
  <!-- <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <img-comparison-slider>
          <img slot="first" src="static/images/carousel1.jpg" />
          <img slot="second" src="static/images/carousel1.jpg" />
        </img-comparison-slider>
      </div>
    </div>
  </section> -->
  <!--End paper poster -->

  <!-- Image comparison grid -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Visual Comparisons</h2>
        <div class="image-comparison-grid">
          <!-- Image Comparison 1 -->
          <div class="image-comparison-item">
            <div class="image-comparison-container">
              <img-comparison-slider hover="hover">
                <figure slot="first" class="before">
                  <img width="100%" src="static/images/before_after/B0.png">
                  <figcaption>Baseline</figcaption>
                </figure>
                <figure slot="second" class="after">
                  <img width="100%" src="static/images/before_after/A0.png">
                  <figcaption>NeRFmented</figcaption>
                </figure>
              </img-comparison-slider>
            </div>
          </div>
          <!-- Image Comparison 2 -->
          <div class="image-comparison-item">
            <div class="image-comparison-container">
              <img-comparison-slider hover="hover">
                <figure slot="first" class="before">
                  <img width="100%" src="static/images/before_after/B1.png">
                  <figcaption>Baseline</figcaption>
                </figure>
                <figure slot="second" class="after">
                  <img width="100%" src="static/images/before_after/A1.png">
                  <figcaption>NeRFmented</figcaption>
                </figure>
              </img-comparison-slider>
            </div>
          </div>
          <!-- Image Comparison 3 -->
          <div class="image-comparison-item">
            <div class="image-comparison-container">
              <img-comparison-slider hover="hover">
                <figure slot="first" class="before">
                  <img width="100%" src="static/images/before_after/B2.png">
                  <figcaption>Baseline</figcaption>
                </figure>
                <figure slot="second" class="after">
                  <img width="100%" src="static/images/before_after/A2.png">
                  <figcaption>NeRFmented</figcaption>
                </figure>
              </img-comparison-slider>
            </div>
          </div>
          <!-- Image Comparison 4 -->
          <div class="image-comparison-item">
            <div class="image-comparison-container">
              <img-comparison-slider hover="hover">
                <figure slot="first" class="before">
                  <img width="100%" src="static/images/before_after/B3.png">
                  <figcaption>Baseline</figcaption>
                </figure>
                <figure slot="second" class="after">
                  <img width="100%" src="static/images/before_after/A3.png">
                  <figcaption>NeRFmented</figcaption>
                </figure>
              </img-comparison-slider>
            </div>
          </div>
        </div>
        <div class="content has-text-justified">
          <p>
            *Baseline is BinsFormer [Li 2022]. NeRFmented is BinsFormer with the interpolation NeRFmentation strategy.
          </p>
        </div>
      </div>
    </div>
  </section>
  <!-- End image comparison grid -->


  <!-- Youtube video -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <!-- Paper video. -->
        <h2 class="title is-3">Video Presentation</h2>
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">

            <div class="publication-video">
              <!-- Youtube embed code here -->
              <iframe src="https://www.youtube.com/embed/ICOT_zkTqmI?si=2veJ5he1iic7NvVF" frameborder="0"
                allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End youtube video -->


  <!-- Paper poster -->
  <!-- <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Poster</h2>

        <iframe src="static/pdfs/sample.pdf" width="100%" height="550">
        </iframe>

      </div>
    </div>
  </section> -->
  <!--End paper poster -->

  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{feldmann2024nerfmentationnerfbasedaugmentationmonocular,
        title={NeRFmentation: NeRF-based Augmentation for Monocular Depth Estimation}, 
        author={Casimir Feldmann and Niall Siegenheim and Nikolas Hars and Lovro Rabuzin and Mert Ertugrul and Luca Wolfart and Marc Pollefeys and Zuria Bauer and Martin R. Oswald},
        year={2024},
        eprint={2401.03771},
        archivePrefix={arXiv},
        primaryClass={cs.CV},
        url={https://arxiv.org/abs/2401.03771}, 
  }</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->

  <!--References -->
  <section class="section" id="References">
    <div class="container is-max-desktop content">
      <h2 class="title">References</h2>
      <div class="content has-text-justified">
        <p>
          [Li 2022] Li, Z., Wang, X., Liu, X., Jiang, J.: Binsformer: Revisiting adaptive bins for monocular depth
          estimation (2022)
        </p>
      </div>
    </div>
  </section>
  <!--End References -->

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the
              footer. <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>